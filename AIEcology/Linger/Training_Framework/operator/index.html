<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.2">
<title data-react-helmet="true">算子列表 | 聆思文档中心</title><meta data-react-helmet="true" property="og:url" content="https://github.com/LISTENAI/docs-csk6/AIEcology/Linger/Training_Framework/operator"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="算子列表 | 聆思文档中心"><meta data-react-helmet="true" name="description" content="目前支持量化的算子"><meta data-react-helmet="true" property="og:description" content="目前支持量化的算子"><link data-react-helmet="true" rel="shortcut icon" href="/docs-csk6/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://github.com/LISTENAI/docs-csk6/AIEcology/Linger/Training_Framework/operator"><link data-react-helmet="true" rel="alternate" href="https://github.com/LISTENAI/docs-csk6/AIEcology/Linger/Training_Framework/operator" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://github.com/LISTENAI/docs-csk6/AIEcology/Linger/Training_Framework/operator" hreflang="x-default"><link rel="stylesheet" href="/docs-csk6/assets/css/styles.b048446a.css">
<link rel="preload" href="/docs-csk6/assets/js/runtime~main.2d336c42.js" as="script">
<link rel="preload" href="/docs-csk6/assets/js/main.70a533e0.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#main" class="skipToContent_vO2r">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" titleclassname="navbar__title" href="/docs-csk6/"><img src="/docs-csk6/img/logo_light.svg" alt="LSOpen Logo" class="themedImage_K3WP themedImage--light_Fy0T navbar__logo"><img src="/docs-csk6/img/logo_dark.svg" alt="LSOpen Logo" class="themedImage_K3WP themedImage--dark_V9oi navbar__logo"></a><a class="navbar__item navbar__link" href="/docs-csk6/chips/4002/Chip_information_4002">芯片</a><a class="navbar__item navbar__link" href="/docs-csk6/tools/LStudio">工具</a><a class="navbar__item navbar__link" href="/docs-csk6/AIsolution/ESR/Quick_start/Scheme_introduction">AI语音应用方案</a><a class="navbar__item navbar__link" href="/docs-csk6/Industrysolution/Scanning_pen/Scheme_introduction">行业Turnkey解决方案</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/docs-csk6/FAQ/front_page">FAQ</a><a class="navbar__item navbar__link" href="/docs-csk6/school/school">视频课程</a><a class="navbar__item navbar__link" href="/docs-csk6/workorder/workorder">工单</a><div class="react-toggle displayOnlyInLargeViewport_a-w0 react-toggle--disabled"><div class="react-toggle-track" role="button" tabindex="-1"><div class="react-toggle-track-check"><span class="toggle_8pH0"><img src="/docs-csk6/img/light.svg" alt="" class="themedImage_K3WP themedImage--light_Fy0T"><img src="/docs-csk6/img/unlight.svg" alt="" class="themedImage_K3WP themedImage--dark_V9oi"></span></div><div class="react-toggle-track-x"><span class="toggle_8pH0"><img src="/docs-csk6/img/undark.svg" alt="" class="themedImage_K3WP themedImage--light_Fy0T"><img src="/docs-csk6/img/dark.svg" alt="" class="themedImage_K3WP themedImage--dark_V9oi"></span></div></div><input type="checkbox" class="react-toggle-screenreader-only" aria-label="Switch between dark and light mode"></div><div class="navbar__search searchBarContainer_I7kZ"><input placeholder="搜索" aria-label="Search" class="navbar__search-input"><div class="loadingRing_Zg7X searchBarLoadingRing_J5Ez"><div></div><div></div><div></div><div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" titleclassname="navbar__title" href="/docs-csk6/"><img src="/docs-csk6/img/logo_light.svg" alt="LSOpen Logo" class="themedImage_K3WP themedImage--light_Fy0T navbar__logo"><img src="/docs-csk6/img/logo_dark.svg" alt="LSOpen Logo" class="themedImage_K3WP themedImage--dark_V9oi navbar__logo"></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/docs-csk6/chips/4002/Chip_information_4002">芯片</a></li><li class="menu__list-item"><a class="menu__link" href="/docs-csk6/tools/LStudio">工具</a></li><li class="menu__list-item"><a class="menu__link" href="/docs-csk6/AIsolution/ESR/Quick_start/Scheme_introduction">AI语音应用方案</a></li><li class="menu__list-item"><a class="menu__link" href="/docs-csk6/Industrysolution/Scanning_pen/Scheme_introduction">行业Turnkey解决方案</a></li><li class="menu__list-item"><a class="menu__link" href="/docs-csk6/FAQ/front_page">FAQ</a></li><li class="menu__list-item"><a class="menu__link" href="/docs-csk6/school/school">视频课程</a></li><li class="menu__list-item"><a class="menu__link" href="/docs-csk6/workorder/workorder">工单</a></li></ul></div></div></div></nav><nav class="navbar subnavbar--fixed-top sub-navbar" style="padding:0"><div class="navbar__inner subnavbar__inner"><div class="navbar__items" style="overflow-x:auto;padding:var(--ifm-navbar-padding-vertical) var(--ifm-navbar-padding-horizontal)"><a class="subnavbar__item subnavbar__link" dirname="/AIEcology" href="/docs-csk6/AIEcology/Intro/intro">总概</a><a aria-current="page" class="subnavbar__item subnavbar__link subnavbar__link--active" dirname="/AIEcology" href="/docs-csk6/AIEcology/Linger/readme">Linger</a><a class="subnavbar__item subnavbar__link" dirname="/AIEcology" href="/docs-csk6/AIEcology/Thinker/readme">Thinker</a></div></div></nav><div class="main-wrapper docs-wrapper doc-page"><div class="docPage_Ol8-"><aside class="docSidebarContainer_UfDv"><div class="sidebar_OMtR"><nav class="menu menu--responsive thin-scrollbar menu_2y2e menuWithAnnouncementBar_Lwkw" aria-label="Sidebar navigation"><button aria-label="Open menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg class="sidebarMenuIcon_IcOF" width="24" height="24" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--first menu__link--sublist" type="first" href="#!">项目介绍</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs-csk6/AIEcology/Linger/Introduction/intro">功能概述</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs-csk6/AIEcology/Linger/Introduction/quick_start">快速开始</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs-csk6/AIEcology/Linger/Introduction/how_to_use">使用方法</a></li></ul></li><li class="menu__list-item menu__list-item--current"><a class="menu__link menu__link--first menu__link--sublist menu__link--active" type="first" href="#!">训练框架</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs-csk6/AIEcology/Linger/Training_Framework/compile">编译方法</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs-csk6/AIEcology/Linger/Training_Framework/train_clamp">clamp训练</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs-csk6/AIEcology/Linger/Training_Framework/train_quant">量化训练</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/docs-csk6/AIEcology/Linger/Training_Framework/operator">算子列表</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs-csk6/AIEcology/Linger/Training_Framework/linger_api">Linger API</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--first menu__link--sublist" type="first" href="#!">功能示例</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs-csk6/AIEcology/Linger/Example/example">功能示例</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--first menu__link--sublist" type="first" href="#!">相关工具</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs-csk6/AIEcology/Linger/Tools/tool">相关工具</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--first menu__link--sublist" type="first" href="#!">贡献内容</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs-csk6/AIEcology/Linger/Contribution/doc">贡献文档说明</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs-csk6/AIEcology/Linger/Contribution/code">贡献代码说明</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--first menu__link--sublist" type="first" href="#!">常见问题</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/docs-csk6/AIEcology/Linger/FAQ/faq">常见问题</a></li></ul></li><li class="menu__list-item"><a class="menu__link" href="/docs-csk6/AIEcology/Linger/readme">README</a></li></ul></nav></div></aside><main class="docMainContainer_dRYS"><div class="container padding-top--md padding-bottom--lg center-container"><div class="row"><div class="col docItemCol_+xOG"><div class="docItemContainer_c+5G"><article><div class="markdown"><header><h1 class="h1Heading_5RMM">算子列表</h1></header><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="目前支持量化的算子"></a>目前支持量化的算子<a class="hash-link" href="#目前支持量化的算子" title="Direct link to heading">#</a></h2><table><thead><tr><th>PyTorch(float32)</th><th>linger算子名称</th><th>linger导出onnx算子名称</th><th>支持关闭的设置</th></tr></thead><tbody><tr><td>nn.BatchNorm2d</td><td><a href="#batchnorm2dint">BatchNorm2dInt</a></td><td>BatchNorm2dInt</td><td>-</td></tr><tr><td>nn.LayerNorm2d</td><td><a href="#layernorm2dint">LayerNorm2dInt</a></td><td>LayerNorm2dInt</td><td>-</td></tr><tr><td>nn.Linear</td><td><a href="#linearint">LinearInt</a></td><td>LinearInt</td><td>-</td></tr><tr><td>nn.Conv1d</td><td><a href="#conv1dint">Conv1dInt</a></td><td>Conv1dInt</td><td>-</td></tr><tr><td>nn.Conv2d</td><td><a href="#conv2dint">Conv2dInt</a></td><td>Conv2dInt</td><td>-</td></tr><tr><td>nn.ConvTranspose2d</td><td><a href="#convtranspose2dint">ConvTranspose2dInt</a></td><td>ConvTranspose2dInt</td><td>-</td></tr><tr><td>nn.AvgPool2d</td><td><a href="#avgpool2dint">AvgPool2dInt</a></td><td>AvgPool2dInt</td><td>-</td></tr><tr><td>nn.MaxPool2d</td><td><a href="#iqMaxPool2d">iqMaxPool2d</a></td><td>MaxPool2d</td><td>-</td></tr><tr><td>nn.GRU</td><td><a href="#gruint">GRUInt</a></td><td>GRUInt/GRUInt_Is8_Is64/GRUInt_Is8_Is64_If32</td><td>-</td></tr><tr><td>nn.LSTM</td><td><a href="#lstmint">LSTMInt</a></td><td>LSTMInt/LSTMInt_Is8_Is64/LSTMInt_Is8_Is64_If32_If32</td><td>-</td></tr><tr><td>nn.Relu</td><td><a href="#relu">iqRelu</a></td><td>Relu</td><td>-</td></tr><tr><td>nn.RELU6</td><td><a href="#reLU6Int">ReLU6Int</a></td><td>Clip</td><td>-</td></tr><tr><td>torch.bmm</td><td><a href="#bmmint">BmmInt</a></td><td>BmmInt</td><td>-</td></tr><tr><td>torch.sigmoid</td><td><a href="#iqsigmoid">iqSigmoid</a></td><td>iqSigmoid</td><td><code>linger.SetIQTensorSigmoid(False)</code></td></tr><tr><td>torch.tanh</td><td><a href="#iqtanh">iqTanh</a></td><td>iqTanh</td><td><code>linger.SetIQTensorTanh(False)</code></td></tr><tr><td>torch.clamp</td><td><a href="#iqclamp">iqClamp</a></td><td>iqClamp</td><td><code>linger.SetIQTensorClamp(False)</code></td></tr><tr><td>torch.cat</td><td><a href="#iqcat">iqCat</a></td><td>iqCat</td><td><code>linger.SetIQTensorCat(False)</code></td></tr><tr><td>torch.transpose</td><td><a href="#iqtranspose">iqTranspose</a></td><td>Transpose</td><td>-</td></tr><tr><td>view</td><td><a href="#iqview">iqView</a></td><td>Reshape</td><td>-</td></tr><tr><td>reshape</td><td><a href="#iqreshape">iqReshape</a></td><td>Reshape</td><td>-</td></tr><tr><td>squeeze</td><td><a href="#iqsqueeze">iqSqueeze</a></td><td>Squeeze</td><td>-</td></tr><tr><td>unsqueeze</td><td><a href="#iqunsqueeze">iqUnsqueeze</a></td><td>Unsqueeze</td><td>-</td></tr><tr><td>flatten</td><td><a href="#iqFlatten">iqFlatten</a></td><td>Flatten</td><td>-</td></tr><tr><td>split</td><td>-</td><td>-</td><td>-</td></tr><tr><td>slice</td><td><a href="#slice">slice</a></td><td>Slice</td><td>-</td></tr><tr><td>sum</td><td><a href="#iqSum">iqSum</a></td><td>iqSum</td><td><code>linger.SetIQTensorSum(False)</code></td></tr><tr><td>add</td><td><a href="#iqadd">iqAdd</a></td><td>iqAdd</td><td><code>linger.SetIQTensorAdd(False)</code></td></tr><tr><td>sub</td><td>-</td><td>-</td><td>-</td></tr><tr><td>mul</td><td><a href="#iqmul">iqMul</a></td><td>iqMul</td><td><code>linger.SetIQTensorMul(False)</code></td></tr><tr><td>div</td><td><a href="#iqDiv">iqDiv</a></td><td>iqDiv</td><td><code>linger.SetIQTensorDiv(False)</code></td></tr><tr><td>upsample</td><td>-</td><td>-</td><td>-</td></tr><tr><td>nn.Embedding</td><td><a href="#EmbeddingInt">EmbeddingInt</a></td><td>Gather</td><td>-</td></tr><tr><td>quant</td><td><a href="#quant">quant</a></td><td>Quant</td><td>-</td></tr><tr><td>dequant</td><td><a href="#dequant">dequant</a></td><td>Dequant</td><td>-</td></tr><tr><td>requant</td><td><a href="#Requant">Requant</a></td><td>Requant</td><td>-</td></tr><tr><td>layernorm</td><td><a href="#LayerNormInt">LayerNormInt</a></td><td>LayerNormInt</td><td>-</td></tr><tr><td>softmax</td><td><a href="#SoftmaxInt">SoftmaxInt</a></td><td>SoftmaxInt</td><td>-</td></tr><tr><td>logsoftmax</td><td><a href="#LogSoftmaxInt">LogSoftmaxInt</a></td><td>LogSoftmaxInt</td><td>-</td></tr><tr><td>flip</td><td><a href="#iqFlip">iqFlip</a></td><td>Slice</td><td>-</td></tr><tr><td>var</td><td><a href="#iqVar">iqVar</a></td><td>iqVar</td><td>-</td></tr><tr><td>-</td><td><a href="#channel_shuffle">channel_shuffle</a></td><td>ShuffleChannel</td><td><code>SetFunctionChannelShuffleQuant(False)</code></td></tr></tbody></table><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="operator-命名规则"></a>Operator 命名规则<a class="hash-link" href="#operator-命名规则" title="Direct link to heading">#</a></h2><p><code>(MajorName)[_Inputs_Outputs]</code></p><ul><li>MajorName 必须要有</li><li>Operator 主名字,名字内部不允许有符号,仅英文和数字。例如iqMul</li></ul><hr><header><h1 class="h1Heading_5RMM">术语说明</h1></header><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="量化方式"></a>量化方式<a class="hash-link" href="#量化方式" title="Direct link to heading">#</a></h2><p>在部分op的属性中有platform_quant属性，标识平台相关量化方法，说明如下:</p><ul><li>luna_quant: castor全量化方式(int8-&gt;int8)，针对castor硬件量化，浮点到定点round采用的(x+0.5).floor()计算</li></ul><p>$$(\lfloor x_int<em>\frac{scale_z}{scale_x}+0.5\rfloor+\lfloor y_int</em>\frac{scale_z}{scale_y}+0.5\rfloor).int().clamp(-128,127)$$</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="scale说明"></a>Scale说明<a class="hash-link" href="#scale说明" title="Direct link to heading">#</a></h2><ul><li>scale_i: Input的scale, scale_x,scale_1,scale_2, scale_y同理, bits可以取8，16等</li></ul><p>$$\frac{2^{bits-1}-1}{running_i}$$</p><ul><li>scale_w: Weight的scale, scale_iw, scale_hw同理, bits可以取8，16等</li></ul><p>$$\frac{2^{bits-1}-1}{weight.abs().max()}$$</p><ul><li>scale_o: Output的scale, bits可以取8，16等</li></ul><p>$$\frac{2^{bits-1}-1}{running_o}$$</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="mode参数值"></a>Mode参数值<a class="hash-link" href="#mode参数值" title="Direct link to heading">#</a></h2><ul><li>mode: 所有模式下的device信息</li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="onnx类型值"></a>onnx类型值<a class="hash-link" href="#onnx类型值" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="类型说明"></a>类型说明<a class="hash-link" href="#类型说明" title="Direct link to heading">#</a></h3><table><thead><tr><th>Group</th><th>Types</th><th>Description</th></tr></thead><tbody><tr><td>Floating Point Types</td><td>FLOAT16, FLOAT32, FLOAT64</td><td>Values adhering to the IEEE 754-2008 standard representation of floating-point data.</td></tr><tr><td>Signed Integer Types</td><td>INT8, INT16, INT32, INT64</td><td>Signed integers are supported for 8-64 bit widths.</td></tr><tr><td>Unsigned Integer Types</td><td>UINT8, UINT16</td><td>Unsigned integers of 8 or 16 bits are supported.</td></tr><tr><td>Complex Types</td><td>COMPLEX64, COMPLEX128</td><td>A complex number with either 32- or 64-bit real and imaginary parts.</td></tr><tr><td>Other</td><td>STRING</td><td>Strings represent textual data. All strings are encoded using UTF-8.</td></tr><tr><td>Other</td><td>BOOL</td><td>Boolean values represent data with only two values, typically true and false.</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="类型和值"></a>类型和值<a class="hash-link" href="#类型和值" title="Direct link to heading">#</a></h3><table><thead><tr><th>类型</th><th>值</th></tr></thead><tbody><tr><td>UNDEFINED</td><td>0</td></tr><tr><td>FLOAT32</td><td>1</td></tr><tr><td>UINT8</td><td>2</td></tr><tr><td>INT8</td><td>3</td></tr><tr><td>UINT16</td><td>4</td></tr><tr><td>INT16</td><td>5</td></tr><tr><td>INT32</td><td>6</td></tr><tr><td>INT64</td><td>7</td></tr><tr><td>STR</td><td>8</td></tr><tr><td>BOOL</td><td>9</td></tr><tr><td>FLOAT16</td><td>10</td></tr><tr><td>UINT32</td><td>12</td></tr><tr><td>UINT64</td><td>13</td></tr><tr><td>COMPLEX64</td><td>14</td></tr><tr><td>COMPLEX128</td><td>15</td></tr><tr><td>BFLOAT16</td><td>16</td></tr></tbody></table><hr><header><h1 class="h1Heading_5RMM">iqAdd</h1></header><p>量化数据加法,由linger导出</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs"></a>Inputs<a class="hash-link" href="#inputs" title="Direct link to heading">#</a></h3><ul><li>x:T,第1个操作tensor</li><li>y:T,第2个操作tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs"></a>Outputs<a class="hash-link" href="#outputs" title="Direct link to heading">#</a></h3><ul><li>o:T,结果</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr"></a>Attr<a class="hash-link" href="#attr" title="Direct link to heading">#</a></h3><ul><li>scale_x:float,required,x的scale</li><li>scale_y:float,required,y的scale</li><li>scale_o:float,required,输出值o的scale</li><li>platform_quant:string,required,支持包括luna_quant，默认为luna_quant</li><li>mode: string, required</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints"></a>Type Constraints<a class="hash-link" href="#type-constraints" title="Direct link to heading">#</a></h3><p>-T:int8,int16,int32</p><hr><header><h1 class="h1Heading_5RMM">iqMul</h1></header><ul><li>量化数据乘法</li><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-1"></a>Inputs<a class="hash-link" href="#inputs-1" title="Direct link to heading">#</a></h3><ul><li>x:T,第1个操作tensor</li><li>y:T,第2个操作tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-1"></a>Outputs<a class="hash-link" href="#outputs-1" title="Direct link to heading">#</a></h3><ul><li>o:T,乘法结果</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-1"></a>Attr<a class="hash-link" href="#attr-1" title="Direct link to heading">#</a></h3><ul><li>scale_x:float,required,x的scale</li><li>scale_y:float,required,y的scale</li><li>scale_o:float,required,输出值o的scale</li><li>platform_quant:string,required,支持包括luna_quant，默认为luna_quant</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-1"></a>Type Constraints<a class="hash-link" href="#type-constraints-1" title="Direct link to heading">#</a></h3><p>-T:tensor(int8),tensor(int16),tensor(int32)</p><hr><header><h1 class="h1Heading_5RMM">iqDiv</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-2"></a>Inputs<a class="hash-link" href="#inputs-2" title="Direct link to heading">#</a></h3><ul><li>x:T,输入tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-2"></a>Outputs<a class="hash-link" href="#outputs-2" title="Direct link to heading">#</a></h3><ul><li>y:T,输出tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-2"></a>Attr<a class="hash-link" href="#attr-2" title="Direct link to heading">#</a></h3><ul><li>input (Tensor) – the dividend</li><li>other (Tensor or Number) – the divisor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-2"></a>Type Constraints<a class="hash-link" href="#type-constraints-2" title="Direct link to heading">#</a></h3><ul><li>T: tensor(int8)</li></ul><hr><header><h1 class="h1Heading_5RMM">EmbeddingInt</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-3"></a>Inputs<a class="hash-link" href="#inputs-3" title="Direct link to heading">#</a></h3><ul><li>x:T,(∗)，任意形状的IntTensor或LongTensor，包含要提取的指数。</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-3"></a>Outputs<a class="hash-link" href="#outputs-3" title="Direct link to heading">#</a></h3><ul><li>y:T, (*, H), 其中*是输入形状，H=embedding_dim</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-3"></a>Attr<a class="hash-link" href="#attr-3" title="Direct link to heading">#</a></h3><ul><li>num_embeddings (int): 嵌入字典的大小</li><li>embedding_dim (int): 每个嵌入向量的大小</li><li>padding_idx (int, optional): 如果指定，padding_idx处的条目不会对梯度做出贡献；因此，padding_idx处的嵌入向量在训练期间不会被更新，也就是说，它仍然是一个固定的 &quot;垫&quot;。对于一个新构建的Embedding，padding_idx处的嵌入向量将默认为全零，但可以更新为另一个值，作为填充向量使用。</li><li>max_norm (float, optional): 如果给定，每一个规范大于max_norm的嵌入向量都会被重新规范化为max_norm的规范。</li><li>norm_type (float, optional): 为max_norm选项计算的p-norm的p。默认为2。</li><li>scale_grad_by_freq (boolean, optional): 如果给定，这将通过迷你批次中单词频率的倒数来扩展梯度。默认为假。</li><li>sparse (bool, optional): 如果为真，梯度与权重矩阵将是一个稀疏张量。</li><li>data_bits: int,required,输入数据bit数，当前仅仅支持8</li><li>scale_x: float,required,输入tensor的scale</li><li>scale_o: float,required,输出tensor的scale</li><li>o_bits: 输出bit数,如果没有该属性,意味着float</li><li>platform_quant: string,required,支持luna_quant</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-3"></a>Type Constraints<a class="hash-link" href="#type-constraints-3" title="Direct link to heading">#</a></h3><ul><li>T: tensor(int8)</li></ul><hr><header><h1 class="h1Heading_5RMM">iqCat</h1></header><ul><li>tensor cat 操作</li><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs1---"></a>Inputs（1 - ∞）<a class="hash-link" href="#inputs1---" title="Direct link to heading">#</a></h3><ul><li>x0:T,第0个tensor</li><li>x1:T,第1个tensor</li><li>x2:T,第2个tensor</li><li>...****</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-4"></a>Outputs<a class="hash-link" href="#outputs-4" title="Direct link to heading">#</a></h3><ul><li>o:T,concat输出tensor</li><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-4"></a>Attr<a class="hash-link" href="#attr-4" title="Direct link to heading">#</a></h3><p><code>个数与inputs相同</code></p><ul><li>scale_x_0:float,required,第0个tensor的scale</li><li>scale_x_1:float,required,第1个tensor的scale</li><li>scale_x_2:float,required,第2个tensor的scale</li><li>...</li><li>dim:int,required,concat的轴，取值[-r, r-1],其中 r = rank(inputs)</li><li>scale_o:float,required,concat后o的tensor</li><li>platform_quant:string,required,平台量化配置，支持包括luna_quant，默认为luna_quant</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-4"></a>Type Constraints<a class="hash-link" href="#type-constraints-4" title="Direct link to heading">#</a></h3><ul><li>T:int8</li></ul><hr><header><h1 class="h1Heading_5RMM">iqtranspose</h1></header><ul><li>矩阵转置</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-4"></a>Inputs<a class="hash-link" href="#inputs-4" title="Direct link to heading">#</a></h3><ul><li>input: 输入tensor</li><li>dim0：input需要转置的维度</li><li>dim1：input需要转置的维度</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="venus-limits"></a>venus limits<a class="hash-link" href="#venus-limits" title="Direct link to heading">#</a></h3><p>tranpose输入不支持4维，2维转置数据大小无限制，3维转置数据大小有限制（假设输入为CHW，数据位宽为data_bytes，限制条件如下所示）
转置组合	硬件限制
(0,2,1)	(W<em>H) </em> data_bytes &lt;= 64KB
(2,0,1)	(W<em>H) </em> data_bytes &lt;= 64KB
(2,1,0)	(W<em>C) </em> data_bytes &lt;= 64KB
(1,2,0)	(W<em>C) </em> data_bytes &lt;= 64KB
(1,0,2)	(W<em>H) </em> data_bytes &lt;= 64KB</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-5"></a>Outputs<a class="hash-link" href="#outputs-5" title="Direct link to heading">#</a></h3><ul><li>转置后的矩阵</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-5"></a>Attr<a class="hash-link" href="#attr-5" title="Direct link to heading">#</a></h3><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-5"></a>Type Constraints<a class="hash-link" href="#type-constraints-5" title="Direct link to heading">#</a></h3><hr><header><h1 class="h1Heading_5RMM">iqview</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-5"></a>Inputs<a class="hash-link" href="#inputs-5" title="Direct link to heading">#</a></h3><ul><li>x:T,输入tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-6"></a>Outputs<a class="hash-link" href="#outputs-6" title="Direct link to heading">#</a></h3><ul><li>y:T,输出tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-6"></a>Attr<a class="hash-link" href="#attr-6" title="Direct link to heading">#</a></h3><ul><li>shape (torch.Size or int...): 希望转换得到的大小</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-6"></a>Type Constraints<a class="hash-link" href="#type-constraints-6" title="Direct link to heading">#</a></h3><ul><li>T: tensor(int8)</li></ul><hr><header><h1 class="h1Heading_5RMM">iqreshape</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-6"></a>Inputs<a class="hash-link" href="#inputs-6" title="Direct link to heading">#</a></h3><ul><li>x:T,输入tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-7"></a>Outputs<a class="hash-link" href="#outputs-7" title="Direct link to heading">#</a></h3><ul><li>y:T,输出tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-7"></a>Attr<a class="hash-link" href="#attr-7" title="Direct link to heading">#</a></h3><ul><li>input (Tensor): the tensor to be reshaped</li><li>shape (tuple of python:ints): the new shape</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-7"></a>Type Constraints<a class="hash-link" href="#type-constraints-7" title="Direct link to heading">#</a></h3><ul><li>T: tensor(int8)</li></ul><hr><header><h1 class="h1Heading_5RMM">iqsqueeze</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-7"></a>Inputs<a class="hash-link" href="#inputs-7" title="Direct link to heading">#</a></h3><ul><li>x:T,输入tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-8"></a>Outputs<a class="hash-link" href="#outputs-8" title="Direct link to heading">#</a></h3><ul><li>y:T,输出tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-8"></a>Attr<a class="hash-link" href="#attr-8" title="Direct link to heading">#</a></h3><ul><li>input (Tensor): the input tensor.</li><li>dim (int, optional): if given, the input will be squeezed only in this dimension</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-8"></a>Type Constraints<a class="hash-link" href="#type-constraints-8" title="Direct link to heading">#</a></h3><ul><li>T: tensor(int8)</li></ul><hr><header><h1 class="h1Heading_5RMM">iqunsqueeze</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-8"></a>Inputs<a class="hash-link" href="#inputs-8" title="Direct link to heading">#</a></h3><ul><li>x:T,输入tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-9"></a>Outputs<a class="hash-link" href="#outputs-9" title="Direct link to heading">#</a></h3><ul><li>y:T,输出tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-9"></a>Attr<a class="hash-link" href="#attr-9" title="Direct link to heading">#</a></h3><ul><li>input (Tensor): the input tensor.</li><li>dim (int, optional): if given, the input will be unsqueezed only in this dimension</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-9"></a>Type Constraints<a class="hash-link" href="#type-constraints-9" title="Direct link to heading">#</a></h3><ul><li>T: tensor(int8)</li></ul><hr><header><h1 class="h1Heading_5RMM">iqFlatten</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-9"></a>Inputs<a class="hash-link" href="#inputs-9" title="Direct link to heading">#</a></h3><ul><li>x:T,输入tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-10"></a>Outputs<a class="hash-link" href="#outputs-10" title="Direct link to heading">#</a></h3><ul><li>y:T,输出tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-10"></a>Attr<a class="hash-link" href="#attr-10" title="Direct link to heading">#</a></h3><ul><li>input (Tensor) – the input tensor.</li><li>start_dim (int) – the first dim to flatten</li><li>end_dim (int) – the last dim to flatten</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-10"></a>Type Constraints<a class="hash-link" href="#type-constraints-10" title="Direct link to heading">#</a></h3><ul><li>T: tensor(int8)</li></ul><hr><header><h1 class="h1Heading_5RMM">iqSum</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-10"></a>Inputs<a class="hash-link" href="#inputs-10" title="Direct link to heading">#</a></h3><ul><li>x:T,输入tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-11"></a>Outputs<a class="hash-link" href="#outputs-11" title="Direct link to heading">#</a></h3><ul><li>y:T,输出tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-11"></a>Type Constraints<a class="hash-link" href="#type-constraints-11" title="Direct link to heading">#</a></h3><ul><li>T: tensor(int8)</li></ul><hr><header><h1 class="h1Heading_5RMM">iqClamp</h1></header><ul><li>数据截断</li><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-11"></a>Inputs<a class="hash-link" href="#inputs-11" title="Direct link to heading">#</a></h3><ul><li>x:T,需要截断的tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-12"></a>Outputs<a class="hash-link" href="#outputs-12" title="Direct link to heading">#</a></h3><ul><li>y:T,截断后的结果 </li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-11"></a>Attr<a class="hash-link" href="#attr-11" title="Direct link to heading">#</a></h3><ul><li>scale_x:float,required,输入x的scale</li><li>scale_o:float,required,输出y的scale</li><li>platform_quant:string,required,平台属性</li><li>min:float,required,clamp最小值</li><li>max:float,required,clamp最大值</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-12"></a>Type Constraints<a class="hash-link" href="#type-constraints-12" title="Direct link to heading">#</a></h3><p>-T:tensor(int8)</p><hr><header><h1 class="h1Heading_5RMM">iqSigmoid</h1></header><ul><li>数据sigmoid激活</li><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-12"></a>Inputs<a class="hash-link" href="#inputs-12" title="Direct link to heading">#</a></h3><ul><li>x:T1,输入tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="venus-limits-1"></a>venus limits<a class="hash-link" href="#venus-limits-1" title="Direct link to heading">#</a></h3><p>Iqsigmoid只支持int16(Q11)输入，int16(Q15)输出	</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-13"></a>Outputs<a class="hash-link" href="#outputs-13" title="Direct link to heading">#</a></h3><ul><li>y:T2,sigmoid后的结果</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-12"></a>Attr<a class="hash-link" href="#attr-12" title="Direct link to heading">#</a></h3><ul><li>scale_x:float,required,输入x的scale</li><li>scale_o:float,required,输出y的scale</li><li>platform_quant:string,required,平台属性</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-13"></a>Type Constraints<a class="hash-link" href="#type-constraints-13" title="Direct link to heading">#</a></h3><ul><li>T1:tensor(int8)</li><li>T2:tensor(uint8)</li></ul><hr><header><h1 class="h1Heading_5RMM">iqTanh</h1></header><ul><li>数据sigmoid激活</li><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-13"></a>Inputs<a class="hash-link" href="#inputs-13" title="Direct link to heading">#</a></h3><ul><li>x:T1,输入tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="venus-limits-2"></a>venus limits<a class="hash-link" href="#venus-limits-2" title="Direct link to heading">#</a></h3><p>iqTanh只支持int16(Q11)输入，int16(Q15)输出	</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-14"></a>Outputs<a class="hash-link" href="#outputs-14" title="Direct link to heading">#</a></h3><ul><li>y:T2,tanh后的结果</li></ul><hr><header><h1 class="h1Heading_5RMM">Relu</h1></header><ul><li>y = max(0, x)</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-14"></a>Inputs<a class="hash-link" href="#inputs-14" title="Direct link to heading">#</a></h3><ul><li>x:T,输入tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-15"></a>Outputs<a class="hash-link" href="#outputs-15" title="Direct link to heading">#</a></h3><ul><li>y:T,relu后的结果</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-14"></a>Type Constraints<a class="hash-link" href="#type-constraints-14" title="Direct link to heading">#</a></h3><ul><li>T:tensor(int8),tensor(int32),tensor(float)</li></ul><hr><header><h1 class="h1Heading_5RMM">ReLU6Int</h1></header><ul><li>ReLU6Int导出为Clip算子，为标准的onnx节点，支持int8的输入输出</li><li>与clamp区别:clip有3个输入，1个输出，即min_thresh和max_thresh作为输入，clamp的min和max是属性</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-15"></a>Inputs<a class="hash-link" href="#inputs-15" title="Direct link to heading">#</a></h3><ul><li>x:T,输入数据tensor</li><li>min_thresh:T,截断的最小值</li><li>max_thresh:T,截断的最大值</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-16"></a>Outputs<a class="hash-link" href="#outputs-16" title="Direct link to heading">#</a></h3><ul><li>y:T,截断后的输出tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-15"></a>Type Constraints<a class="hash-link" href="#type-constraints-15" title="Direct link to heading">#</a></h3><ul><li>T:tensor(int8), tensor(float)</li></ul><hr><header><h1 class="h1Heading_5RMM">AvgPool2dInt</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-16"></a>Inputs<a class="hash-link" href="#inputs-16" title="Direct link to heading">#</a></h3><ul><li>x:T,格式(N x C x H x W),输入tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-17"></a>Outputs<a class="hash-link" href="#outputs-17" title="Direct link to heading">#</a></h3><ul><li>y:T,格式(N x C x H x W),输出tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-13"></a>Attr<a class="hash-link" href="#attr-13" title="Direct link to heading">#</a></h3><ul><li>kernel_shape:int2,required,pool2d 的kernel大小</li><li>strides:int2,required,pool2d 的stride</li><li>pads:int2,required,pool2d的pad大小</li><li>ceil_mode:bool,是否为ceil模式</li><li>data_bits:int,required,输入数据bit数，当前仅仅支持8</li><li>scale_x:float,required,输入tensor的scale</li><li>scale_o:float,required,输出tensor的scale</li><li>o_bits:输出bit数,如果没有该属性,意味着float</li><li>platform_quant:string,required,支持luna_quant</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-16"></a>Type Constraints<a class="hash-link" href="#type-constraints-16" title="Direct link to heading">#</a></h3><ul><li>T: tensor(int8)</li></ul><hr><header><h1 class="h1Heading_5RMM">iqMaxPool2d</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-17"></a>Inputs<a class="hash-link" href="#inputs-17" title="Direct link to heading">#</a></h3><ul><li>x:T,格式(N x C x H x W),输入tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-18"></a>Outputs<a class="hash-link" href="#outputs-18" title="Direct link to heading">#</a></h3><ul><li>y:T,格式(N x C x H x W),输出tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-14"></a>Attr<a class="hash-link" href="#attr-14" title="Direct link to heading">#</a></h3><ul><li>kernel_size:int2,required,pool2d 的kernel大小</li><li>stride:int2,required,pool2d 的stride</li><li>padding:int2,required,pool2d的pad大小</li><li>ceil_mode:bool,是否为ceil模式</li><li>dilation:默认为1</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-17"></a>Type Constraints<a class="hash-link" href="#type-constraints-17" title="Direct link to heading">#</a></h3><ul><li>T: tensor(int8)</li></ul><hr><header><h1 class="h1Heading_5RMM">Conv2dInt</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-18"></a>Inputs<a class="hash-link" href="#inputs-18" title="Direct link to heading">#</a></h3><ul><li>x:T1,格式(N X C X H X W),卷积的激活值</li><li>weight:T1,格式(M x C/group x kH x kW),M是feature maps数量,C是channels数量,kH和kW是feature map的高和长</li><li>bias:T2,optional,1D bias</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="venus-limits-3"></a>venus limits<a class="hash-link" href="#venus-limits-3" title="Direct link to heading">#</a></h3><ul><li><p>kernel大小为1-5（kernel_h、kernel_w设置相互独立）	</p></li><li><p>stride大小为1/2/4（stride_h 、stride_w设置相互独立）	</p></li><li><p>pad大小为0-4（四个方向上的pad设置相互独立）	</p></li><li><p>输入数据对齐后大小不超过64KB（channel按8字节对齐，w按照8*stride_w字节对齐，channel不能超过一定阈值（待定））	</p></li><li><p>weight对齐后数据大小不超过32KB（非depthwise卷积的channel_out按2字节对齐，channel_in按8字节对齐。Depthwise卷积的channel_in按16字节对齐）	</p></li><li><p>输入数据和weight之间的组合	</p><ul><li>in_w &gt;= weight_w &amp;&amp; in_h &gt;= weight_h	</li><li>weight_w &gt;= stride_w &amp;&amp; weight_h &gt;= stride_h	</li><li>pad_h_up &lt; weight_h &amp;&amp; pad_h_down &lt; weight_h	</li><li>pad_w_left &lt; weight_w &amp;&amp; pad_w_right &lt; weight_w	</li></ul></li><li><p>输入数据和weight只支持int8，bias为32bit，输出支持int8/int16/int32	</p></li><li><p>max_pool只支持输入输出都为int8	</p></li><li><p>average_pool只支持输入int8，输出int8/int16	</p></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-19"></a>Outputs<a class="hash-link" href="#outputs-19" title="Direct link to heading">#</a></h3><ul><li>o:T3,格式(N X C X H X W),卷积后的输出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-15"></a>Attr<a class="hash-link" href="#attr-15" title="Direct link to heading">#</a></h3><ul><li>dilations:int or int2,required</li><li>group:int,required,输入到输出的卷积块数</li><li>kernel_shape:int or int2,required,卷积核大小</li><li>pads:int or int2,required,两边pad 0的大小</li><li>strides:int or int2,required,卷积的stride</li><li>scale_x:float,required,输入x的feature maps的scale</li><li>scale_w:float,required,weight的scale</li><li>scale_o:float,optional,输出o的scale,没有该属性意味着浮点输出</li><li>data_bits:int,required,x的量化bit数,比如8 表示8bit量化的</li><li>parameter_bits:int,required,weight的量化bit数,比如8 表示8bit量化的</li><li>o_bits:int,optional,输出的o的量化bit数,比如8 表示8bit量化的,没有该属性意味着浮点输出</li><li>platform_quant:string,平台属性,luna_quant, mlu_quant,gpu_quant,
<code>如果linger处设置platform_quant为mlu_quant/gpu_quant,则out_bits=None,onnx中则不会有o_bits属性</code></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-18"></a>Type Constraints<a class="hash-link" href="#type-constraints-18" title="Direct link to heading">#</a></h3><ul><li>T1:tensor(int8)</li><li>T2:tensor(int16),tensor(int32),tensor(float)</li><li>T3:tensor(int8),tensor(float)</li></ul><hr><header><h1 class="h1Heading_5RMM">ConvTranspose2dInt</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-19"></a>Inputs<a class="hash-link" href="#inputs-19" title="Direct link to heading">#</a></h3><ul><li>x:T1,格式(N x C x H x W),输入反卷积数据</li><li>weight:T1,格式(C x M/group x kH x kW),反卷积的weight,M是feature map数,C是通道数，kH和kW是feaaturemap的高和长</li><li>bias:T2,optional,1D bias</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="venus-limits-4"></a>venus limits<a class="hash-link" href="#venus-limits-4" title="Direct link to heading">#</a></h3><p>deconv独有的限制	</p><ul><li>stride_h = 2, kernel_h = 2/3/4/5	</li><li>stride_h = 4, kernel_h = 4/5	</li><li>stirde_w = 2, kernel_w= 2/3/4/5	</li><li>stride_w = 4, kernel_w = 4/5	</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-20"></a>Outputs<a class="hash-link" href="#outputs-20" title="Direct link to heading">#</a></h3><ul><li>o:T3,格式(N x C x H x W),反卷积结果</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-16"></a>Attr<a class="hash-link" href="#attr-16" title="Direct link to heading">#</a></h3><ul><li>dilations:int or int2,required</li><li>group:int,required,输入到输出的反卷积块数</li><li>kernel_shape:int or int2,required,反卷积核大小</li><li>pads:int or int2,required,两边pad 0的大小,<code>dilation * (kernel_size - 1) - padding</code></li><li>strides:int or int2,required,反卷积的stride</li><li>output_padding:int or int2,required,反卷积输出的额外pad大小  </li><li>scale_x:float,required,输入x的feature maps的scale</li><li>scale_w:float,required,weight的scale</li><li>scale_o:float,optional,输出o的scale,没有该属性意味着浮点输出</li><li>data_bits:int,required,x的量化bit数,比如8 表示8bit量化的</li><li>parameter_bits:int,required,weight的量化bit数,比如8 表示8bit量化的</li><li>o_bits:int,optional,输出的o的量化bit数,比如8 表示8bit量化的,没有该属性意味着浮点输出</li><li>platform_quant:string,平台属性,luna_quant, mlu_quant,gpu_quant,
<code>如果linger处设置platform_quant为mlu_quant/gpu_quant,则out_bits=None,onnx中则不会有o_bits属性</code></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-19"></a>Type Constraints<a class="hash-link" href="#type-constraints-19" title="Direct link to heading">#</a></h3><ul><li>T1:tensor(int8)</li><li>T2:tensor(int16),tensor(int32),tensor(float)</li><li>T3:tensor(int8),tensor(float)</li></ul><hr><header><h1 class="h1Heading_5RMM">BatchNorm2dInt</h1></header><ul><li>linger导出</li><li>算法:<code>o = x * mul_w + add_b</code></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-20"></a>Inputs<a class="hash-link" href="#inputs-20" title="Direct link to heading">#</a></h3><ul><li>x:T,格式(N x C X H x W),batchnorm的输入feature maps</li><li>mul_w:T,batch_norm 化简后的乘法系数</li><li>add_b:T,batch_norm 化简后的加法系数</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-21"></a>Outputs<a class="hash-link" href="#outputs-21" title="Direct link to heading">#</a></h3><ul><li>o:T,格式(N x C X H x W),输出tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-17"></a>Attr<a class="hash-link" href="#attr-17" title="Direct link to heading">#</a></h3><ul><li>scale_mul_x:float,required,乘法操作的x的scale</li><li>scale_mul_w:float,required,乘法操作的w的scale</li><li>scale_mul_o:float,required,乘法输出的scale</li><li>scale_add_b:float,required,加法的weight b的scale</li><li>scale_add_o:float,required,输出o的scale</li><li>data_bits:int,required,输入数据bit数</li><li>parameter_bits:int,required,默认为8</li><li>o_bits:int,required,输出bit数,也代表着中间乘法操作后(加法前)的中间运算bit数</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-20"></a>Type Constraints<a class="hash-link" href="#type-constraints-20" title="Direct link to heading">#</a></h3><ul><li>T:tensor(int8),tensor(int16),tensor(int32),</li></ul><hr><header><h1 class="h1Heading_5RMM">LayerNorm2dInt</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-21"></a>Inputs<a class="hash-link" href="#inputs-21" title="Direct link to heading">#</a></h3><ul><li>x:T,格式(N x C X H x W),batchnorm的输入feature maps</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-22"></a>Outputs<a class="hash-link" href="#outputs-22" title="Direct link to heading">#</a></h3><ul><li>o:T,格式(N x C X H x W),输出tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-18"></a>Attr<a class="hash-link" href="#attr-18" title="Direct link to heading">#</a></h3><ul><li>scale_mul_x:float,required,乘法操作的x的scale</li><li>scale_mul_w:float,required,乘法操作的w的scale</li><li>scale_mul_o:float,required,乘法输出的scale</li><li>scale_add_b:float,required,加法的weight b的scale</li><li>scale_add_o:float,required,输出o的scale</li><li>data_bits:int,required,输入数据bit数</li><li>parameter_bits:int,required,默认为8</li><li>o_bits:int,required,输出bit数,也代表着中间乘法操作后(加法前)的中间运算bit数</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-21"></a>Type Constraints<a class="hash-link" href="#type-constraints-21" title="Direct link to heading">#</a></h3><ul><li>T:tensor(int8),tensor(int16),tensor(int32),</li></ul><hr><header><h1 class="h1Heading_5RMM">LinearInt</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-22"></a>Inputs<a class="hash-link" href="#inputs-22" title="Direct link to heading">#</a></h3><ul><li>x:T1,格式(B x M x K),输入全连接数据</li><li>weight:T1,格式(K x N),全连接的weight</li><li>bias:T2,optional,1D bias</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="venus-limits-5"></a>venus limits<a class="hash-link" href="#venus-limits-5" title="Direct link to heading">#</a></h3><p>linearint/matmul左边输入矩阵对齐后大小不超过64KB（假设左矩阵维度为M*N），右矩阵不做限制	</p><ul><li>数据类型为8bit时，M按4字节对齐，N按8字节对齐。	</li><li>数据类型为16bit时，M按4字节对齐，N按2字节对齐。	</li><li>数据类型为32bit时，M按2字节对齐，N按2字节对齐。	</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-23"></a>Outputs<a class="hash-link" href="#outputs-23" title="Direct link to heading">#</a></h3><ul><li>o:T3,格式(B x N),全连接</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-19"></a>Attr<a class="hash-link" href="#attr-19" title="Direct link to heading">#</a></h3><ul><li>scale_x:float,required,输入x的feature maps的scale</li><li>scale_w:float,required,weight的scale</li><li>scale_o:float,optional,输出o的scale,没有该属性意味着浮点输出</li><li>data_bits:int,required,x的量化bit数,比如8 表示8bit量化的</li><li>parameter_bits:int,required,weight的量化bit数,比如8 表示8bit量化的</li><li>o_bits:int,optional,输出的o的量化bit数,比如8 表示8bit量化的,没有该属性意味着浮点输出</li><li>platform_quant:string,luna_quant, mlu_quant,gpu_quant,
<code>如果linger处设置platform_quant为mlu_quant/gpu_quant,则out_bits=None,onnx中则不会有o_bits属性</code></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-22"></a>Type Constraints<a class="hash-link" href="#type-constraints-22" title="Direct link to heading">#</a></h3><ul><li>T1:tensor(int8)</li><li>T2:tensor(int16),tensor(int32),tensor(float)</li><li>T3:tensor(int8),tensor(float)</li></ul><hr><header><h1 class="h1Heading_5RMM">LSTMInt</h1></header><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-23"></a>Inputs<a class="hash-link" href="#inputs-23" title="Direct link to heading">#</a></h3><ul><li>x:T1,格式(B x T x D)或者(T x B x D),输入数据,B 是batch，T是time，D是input dim</li><li>weight_ih:T1,输入连接的weight</li><li>weight_hh:T1,hidden连接的weight</li><li>bias_ih:T2,输入连接的weight后的bias</li><li>bias_hh:T2,hidden连接的weight后的bias</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-24"></a>Outputs<a class="hash-link" href="#outputs-24" title="Direct link to heading">#</a></h3><ul><li>output:有o_bits属性为T3，没有为T4</li><li>hidden_state:T4</li><li>cell_state:T4</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-20"></a>Attr<a class="hash-link" href="#attr-20" title="Direct link to heading">#</a></h3><ul><li>scale_i:float,required,输入数据scale</li><li>scale_h:float,required,hidden scale</li><li>scale_iw:float,required,weight_ih的scale</li><li>scale_hw:float,required,weight_hh的scale</li><li>scale_io:float,optional,输入矩阵计算(Wi*Xi+Bi)的输出量化scale</li><li>scale_ho:float,optional,隐层矩阵计算(Wh*H+Bh)的输出量化scale</li><li>scale_o:float,optional,如果o_bits没有,scale_o为空</li><li>o_bits:int,optional,觉得输出是否做量化</li><li>platform_quant:string,required,对应不同的硬件平台</li><li>data_bits:int,required，输入量化bit数</li><li>parameter_bits:int,required,weight_iw,weight_hw的数据位数</li><li>batch_first:int,required,1表明输入数据是否是B<em>T</em>D模式，0表明是T<em>B</em>D输入格式</li><li>dropout:float,required,对应标准lstm中的dropout操作，量化是全部为0，不做dropout操作</li><li>go_gorward:int,required,针对双向lstm的量化导出，1表示正向，0表示反向</li><li>num_layers:int,required,量化只支持num_layers=1</li><li>input_size:int,required,输入数据维度</li><li>hidden_size:int,required,隐层状态维度</li><li>table_len:int,optional,如果使用查表法,有此属性,表示表长度</li><li>sigmoid_bound:float,optional,sigmoid查表计算的查表边界</li><li>tanh_bound:float,optional,tanh查表计算的查表边界</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-23"></a>Type Constraints<a class="hash-link" href="#type-constraints-23" title="Direct link to heading">#</a></h3><ul><li><p>T1:tensor(int8)</p></li><li><p>T2:tensor(int32)</p></li><li><p>T3:tensor(int8)</p></li><li><p>T4:tensor(float32)</p><p><img src="/docs-csk6/assets/images/lstm_int-008981d38bfb11aff8c7629dd9dfa4cc.png"></p></li></ul><hr><header><h1 class="h1Heading_5RMM">LSTMInt_Is8_Is64</h1></header><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-24"></a>Inputs<a class="hash-link" href="#inputs-24" title="Direct link to heading">#</a></h3><ul><li>x:T1,格式(B x T x D)或者(T x B x D),输入数据,B 是batch，T是time，D是input dim</li><li>batch_seq:T5,输入x的长度tensor，表示成list为[T0,T1,...],T0表示输入x的第0个tensor的T的实际长度,list长度为B</li><li>weight_ih:T1,输入连接的weight</li><li>weight_hh:T1,hidden连接的weight</li><li>bias_ih:T2,输入连接的weight后的bias</li><li>bias_hh:T2,hidden连接的weight后的bias</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-25"></a>Outputs<a class="hash-link" href="#outputs-25" title="Direct link to heading">#</a></h3><ul><li>output:有o_bits属性为T3，没有为T4</li><li>hidden_state:T4</li><li>cell_state:T4</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-21"></a>Attr<a class="hash-link" href="#attr-21" title="Direct link to heading">#</a></h3><ul><li>和LSTMInt一致</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-24"></a>Type Constraints<a class="hash-link" href="#type-constraints-24" title="Direct link to heading">#</a></h3><ul><li><p>T1:tensor(int8)</p></li><li><p>T2:tensor(int32)</p></li><li><p>T3:tensor(int8)</p></li><li><p>T4:tensor(float32)</p></li><li><p>T5:tensor(int64)</p><p><img src="/docs-csk6/assets/images/lstm_int_with_batch_length-7819f16f2f587e33720ace27a0eeef59.png"></p></li></ul><hr><header><h1 class="h1Heading_5RMM">LSTMInt_Is8_Is64_If32_If32</h1></header><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-25"></a>Inputs<a class="hash-link" href="#inputs-25" title="Direct link to heading">#</a></h3><ul><li>x:T1,格式(B x T x D)或者(T x B x D),输入数据,B 是batch，T是time，D是input dim</li><li>batch_seq:T5,输入x的长度tensor，表示成list为[T0,T1,...],T0表示输入x的第0个tensor的T的实际长度,list长度为B</li><li>hidden_state:T4,隐层单元输入</li><li>cell_state:T4,记忆单元输入</li><li>weight_ih:T1,输入连接的weight</li><li>weight_hh:T1,hidden连接的weight</li><li>bias_ih:T2,输入连接的weight后的bias</li><li>bias_hh:T2,hidden连接的weight后的bias</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-26"></a>Outputs<a class="hash-link" href="#outputs-26" title="Direct link to heading">#</a></h3><ul><li>output:有o_bits属性为T3，没有为T4</li><li>hidden_state:T4</li><li>cell_state:T4</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-22"></a>Attr<a class="hash-link" href="#attr-22" title="Direct link to heading">#</a></h3><ul><li>和LSTMInt一致</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-25"></a>Type Constraints<a class="hash-link" href="#type-constraints-25" title="Direct link to heading">#</a></h3><ul><li><p>T1:tensor(int8)</p></li><li><p>T2:tensor(int32)</p></li><li><p>T3:tensor(int8)</p></li><li><p>T4:tensor(float32)</p></li><li><p>T5:tensor(int64)</p><p><img src="/docs-csk6/assets/images/lstm_int_with_batch_length_and_state-f3e68bfdcacc9855df374b7de47c6e22.png"></p></li></ul><hr><header><h1 class="h1Heading_5RMM">GRUInt</h1></header><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-26"></a>Inputs<a class="hash-link" href="#inputs-26" title="Direct link to heading">#</a></h3><ul><li>x:T1,格式(B x T x D)或(T x B x D),输入数据,B 是batch，T是time，D是input dim</li><li>weight_ih:T1,输入连接的weight</li><li>weight_hh:T1,hidden连接的weight</li><li>bias_ih:T2,输入连接的weight后的bias</li><li>bias_hh:T2,hidden连接的weight后的bias</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-27"></a>Outputs<a class="hash-link" href="#outputs-27" title="Direct link to heading">#</a></h3><ul><li>output:T3</li><li>hidden:T4</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-23"></a>Attr<a class="hash-link" href="#attr-23" title="Direct link to heading">#</a></h3><ul><li>scale_i:float,required,输入数据scale</li><li>scale_h:float,required,hidden scale</li><li>scale_iw:float,required,weight_ih的scale</li><li>scale_hw:float,required,weight_hh的scale</li><li>scale_io:float,optional,输入矩阵计算(Wi*Xi+Bi)的输出量化scale</li><li>scale_ho:float,optional,隐层矩阵计算(Wh*H+Bh)的输出量化scale</li><li>scale_o:float,optional,如果o_bits没有,scale_o为空</li><li>o_bits:int,optional,觉得输出是否做量化</li><li>platform_quant:string,required</li><li>data_bits:int,required</li><li>parameter_bits:int,required,weight_iw,weight_hw的数据位数</li><li>batch_first:int,required,1表明输入数据是否是B<em>T</em>D模式，0表明是T<em>B</em>D输入格式</li><li>dropout:float,required,对应标准lstm中的dropout操作，量化是全部为0，不做dropout操作</li><li>go_gorward:int,required,针对双向lstm的量化导出，1表示正向，0表示反向</li><li>num_layers:int,required,量化只支持num_layers=1</li><li>input_size:int,required,输入数据维度</li><li>hidden_size:int,required,隐层状态维度</li><li>table_len:int,optional,如果使用查表法,有此属性,表示表长度</li><li>sigmoid_bound:float,optional,sigmoid查表计算的查表边界</li><li>tanh_bound:float,optional,tanh查表计算的查表边界</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-26"></a>Type Constraints<a class="hash-link" href="#type-constraints-26" title="Direct link to heading">#</a></h3><ul><li><p>T1:tensor(int8)</p></li><li><p>T2:tensor(int32)</p></li><li><p>T3:tensor(int8)</p></li><li><p>T4:tensor(float32)</p></li><li><p>T5:tensor(int64)</p><p><img src="/docs-csk6/assets/images/gru_int-10e69eb1fc8cce7907d4440bbd0ecf89.png"></p></li></ul><hr><header><h1 class="h1Heading_5RMM">GRUInt_Is8_Is64</h1></header><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-27"></a>Inputs<a class="hash-link" href="#inputs-27" title="Direct link to heading">#</a></h3><ul><li>x:T1,格式(B x T x D)或(T x B x D),输入数据,B 是batch，T是time，D是input dim</li><li>batch_seq:T5,输入x的长度tensor，表示成list为[T0,T1,...],T0表示输入x的第0个tensor的T的实际长度,list长度为B</li><li>weight_ih:T1,输入连接的weight</li><li>weight_hh:T1,hidden连接的weight</li><li>bias_ih:T2,输入连接的weight后的bias</li><li>bias_hh:T2,hidden连接的weight后的bias</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-28"></a>Outputs<a class="hash-link" href="#outputs-28" title="Direct link to heading">#</a></h3><ul><li>output:T3</li><li>hidden:T4</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-24"></a>Attr<a class="hash-link" href="#attr-24" title="Direct link to heading">#</a></h3><ul><li>与GRUInt一致</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-27"></a>Type Constraints<a class="hash-link" href="#type-constraints-27" title="Direct link to heading">#</a></h3><ul><li><p>T1:tensor(int8)</p></li><li><p>T2:tensor(int32)</p></li><li><p>T3:tensor(int8)</p></li><li><p>T4:tensor(float32)</p></li><li><p>T5:tensor(int64)</p><p><img src="/docs-csk6/assets/images/gru_int_with_batch_length-3caf6d402b061b664f30480732b9a631.png"></p></li></ul><hr><header><h1 class="h1Heading_5RMM">GRUInt_Is8_Is64_If32</h1></header><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-28"></a>Inputs<a class="hash-link" href="#inputs-28" title="Direct link to heading">#</a></h3><ul><li>x:T1,格式(B x T x D)或(T x B x D),输入数据,B 是batch，T是time，D是input dim</li><li>batch_seq:T5,输入x的长度tensor，表示成list为[T0,T1,...],T0表示输入x的第0个tensor的T的实际长度,list长度为B</li><li>hidden_state:T4,隐层输入状态</li><li>weight_ih:T1,输入连接的weight</li><li>weight_hh:T1,hidden连接的weight</li><li>bias_ih:T2,输入连接的weight后的bias</li><li>bias_hh:T2,hidden连接的weight后的bias</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-29"></a>Outputs<a class="hash-link" href="#outputs-29" title="Direct link to heading">#</a></h3><ul><li>output:T3</li><li>hidden_state:T4</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-25"></a>Attr<a class="hash-link" href="#attr-25" title="Direct link to heading">#</a></h3><ul><li>与GRUInt一致</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-28"></a>Type Constraints<a class="hash-link" href="#type-constraints-28" title="Direct link to heading">#</a></h3><ul><li><p>T1:tensor(int8)</p></li><li><p>T2:tensor(int32)</p></li><li><p>T3:tensor(int8)</p></li><li><p>T4:tensor(float32)</p></li><li><p>T5:tensor(int64)</p><p><img src="/docs-csk6/assets/images/gru_int_with_batch_length_with_state-57663f8167a8feff957ec33c9b4331a9.png"></p></li></ul><hr><header><h1 class="h1Heading_5RMM">Quant</h1></header><ul><li>Quant主要是用来实现浮点输出转下一层定点输入的连接</li><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-29"></a>Inputs<a class="hash-link" href="#inputs-29" title="Direct link to heading">#</a></h3><ul><li>x:T1,要量化的float的tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-30"></a>Outputs<a class="hash-link" href="#outputs-30" title="Direct link to heading">#</a></h3><ul><li>y:T2,量化后的tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-26"></a>Attr<a class="hash-link" href="#attr-26" title="Direct link to heading">#</a></h3><ul><li>data_bits:int,required,量化bit数，当前支持小于等于8</li><li>scale_x:float,required,量化的scale</li><li>platform_quant:string,required,support luna_quant,其他方式策略一致</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-29"></a>Type Constraints<a class="hash-link" href="#type-constraints-29" title="Direct link to heading">#</a></h3><ul><li>T1:tensor(float)</li><li>T2:tensor(int8)</li></ul><hr><header><h1 class="h1Heading_5RMM">Dequant</h1></header><ul><li>Dequant主要是用来实现定点输出转下一层浮点输入的连接</li><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-30"></a>Inputs<a class="hash-link" href="#inputs-30" title="Direct link to heading">#</a></h3><ul><li>x:T1,输入定点tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-31"></a>Outputs<a class="hash-link" href="#outputs-31" title="Direct link to heading">#</a></h3><ul><li>y:T2,输出浮点tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-27"></a>Attr<a class="hash-link" href="#attr-27" title="Direct link to heading">#</a></h3><ul><li>scale_o:float,required,浮点转定点的scale</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-30"></a>Type Constraints<a class="hash-link" href="#type-constraints-30" title="Direct link to heading">#</a></h3><ul><li>T1:tensor(int8)</li><li>T2:tensor(float)</li></ul><hr><header><h1 class="h1Heading_5RMM">BmmInt</h1></header><ul><li>用于torch.bmm的量化训练导出</li><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-31"></a>Inputs<a class="hash-link" href="#inputs-31" title="Direct link to heading">#</a></h3><ul><li>x: T, 输入数据tensor, shape = (B<em>N</em>M)</li><li>y: T, 输入数据tensor, shape = (B<em>M</em>P)</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-32"></a>Outputs<a class="hash-link" href="#outputs-32" title="Direct link to heading">#</a></h3><ul><li>outputs:有o_bits属性为T，否则为T1</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-28"></a>Attr<a class="hash-link" href="#attr-28" title="Direct link to heading">#</a></h3><ul><li>data_bits:int,required,输入数据量化bit位</li><li>o_bits:int,optional,输出数据量化bit位 </li><li>platform_quant:str, required 量化硬件平台参数</li><li>scale_x:float,required,输入x的量化scale</li><li>scale_y:float,required,输入y的量化scale</li><li>scale_o:float,optional,输出out的量化scale</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-31"></a>Type Constraints<a class="hash-link" href="#type-constraints-31" title="Direct link to heading">#</a></h3><ul><li><p>T:tensor(int8)</p></li><li><p>T1:tensor(float)</p><p><img src="/docs-csk6/assets/images/bmm_int-a6396c67925a359fa34c0b79bf0a7a5d.png"></p></li></ul><header><h1 class="h1Heading_5RMM">layernormint</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-32"></a>Inputs<a class="hash-link" href="#inputs-32" title="Direct link to heading">#</a></h3><ul><li>x:T,输入：(N，*)</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-33"></a>Outputs<a class="hash-link" href="#outputs-33" title="Direct link to heading">#</a></h3><ul><li>y:T,输出, (N, *) (与输入相同的形状)</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-29"></a>Attr<a class="hash-link" href="#attr-29" title="Direct link to heading">#</a></h3><ul><li>normalized_shape (int or list or torch.Size)-
预期输入的大小形状
[\∗ × normalized_shape[0] × normalized_shape[1] ×...× normalized_shape[-1]]
如果使用一个整数，它将被视为一个单子列表，本模块将在最后一个维度上进行归一化处理，该维度预计为该特定尺寸。</li><li>eps - 为了数值的稳定性而加到分母上的一个值。默认值：1e-5</li><li>elementwise_affine - 一个布尔值，当设置为 &quot;True &quot;时，该模块具有可学习的每元素仿生参数，初始化为1（用于权重）和0（用于偏差）</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-32"></a>Type Constraints<a class="hash-link" href="#type-constraints-32" title="Direct link to heading">#</a></h3><ul><li>T: tensor(int8)</li></ul><hr><header><h1 class="h1Heading_5RMM">SoftmaxInt</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-33"></a>Inputs<a class="hash-link" href="#inputs-33" title="Direct link to heading">#</a></h3><ul><li>x:T,(∗) 其中*表示，任何数量的附加维度</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-34"></a>Outputs<a class="hash-link" href="#outputs-34" title="Direct link to heading">#</a></h3><ul><li>y:T,(*)，与输入的形状相同</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-30"></a>Attr<a class="hash-link" href="#attr-30" title="Direct link to heading">#</a></h3><ul><li>dim（int）--计算Softmax的维度（因此沿dim的每个片断的总和都是1）。</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-33"></a>Type Constraints<a class="hash-link" href="#type-constraints-33" title="Direct link to heading">#</a></h3><ul><li>T: tensor(int8)</li></ul><hr><header><h1 class="h1Heading_5RMM">LogSoftmaxInt</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-34"></a>Inputs<a class="hash-link" href="#inputs-34" title="Direct link to heading">#</a></h3><ul><li>x:T,(∗) 其中*表示，任何数量的附加维度</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-35"></a>Outputs<a class="hash-link" href="#outputs-35" title="Direct link to heading">#</a></h3><ul><li>y:T,(*)，与输入的形状相同</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-31"></a>Attr<a class="hash-link" href="#attr-31" title="Direct link to heading">#</a></h3><ul><li>dim（int）--计算logSoftmax的维度（因此沿dim的每个片断的总和都是1）。</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-34"></a>Type Constraints<a class="hash-link" href="#type-constraints-34" title="Direct link to heading">#</a></h3><ul><li>T: tensor(int8)</li></ul><hr><header><h1 class="h1Heading_5RMM">iqflip</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-35"></a>Inputs<a class="hash-link" href="#inputs-35" title="Direct link to heading">#</a></h3><ul><li>x:T,输入的tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-36"></a>Outputs<a class="hash-link" href="#outputs-36" title="Direct link to heading">#</a></h3><ul><li>y:T,输出的tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="attr-32"></a>Attr<a class="hash-link" href="#attr-32" title="Direct link to heading">#</a></h3><ul><li>dim（int）-- 指定翻转的的维度</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-35"></a>Type Constraints<a class="hash-link" href="#type-constraints-35" title="Direct link to heading">#</a></h3><ul><li>T: tensor(int8)</li></ul><header><h1 class="h1Heading_5RMM">iqVar</h1></header><ul><li>linger导出</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-36"></a>Inputs<a class="hash-link" href="#inputs-36" title="Direct link to heading">#</a></h3><ul><li>x:T,输入的tensor</li><li>dim: 维度</li><li>unbiased: 无偏/有偏</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-37"></a>Outputs<a class="hash-link" href="#outputs-37" title="Direct link to heading">#</a></h3><ul><li>y:T,输出的tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-36"></a>Type Constraints<a class="hash-link" href="#type-constraints-36" title="Direct link to heading">#</a></h3><ul><li>T: tensor(int8)</li></ul><header><h1 class="h1Heading_5RMM">channel_shuffle</h1></header><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="inputs-37"></a>Inputs<a class="hash-link" href="#inputs-37" title="Direct link to heading">#</a></h3><ul><li>x:T,输入的tensor</li><li>groups: 分组数量</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="outputs-38"></a>Outputs<a class="hash-link" href="#outputs-38" title="Direct link to heading">#</a></h3><ul><li>y:T,输出的tensor</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_k6ZT" id="type-constraints-37"></a>Type Constraints<a class="hash-link" href="#type-constraints-37" title="Direct link to heading">#</a></h3><ul><li>T: tensor(int8)</li></ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs-csk6/AIEcology/Linger/Training_Framework/train_quant"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">« 量化训练</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs-csk6/AIEcology/Linger/Training_Framework/linger_api"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Linger API »</div></a></div></nav><div class="gitalk-wrapper"><ul class="gitalk-option"><li class="like"><i class="icon like-icon"></i> <span id="like_num">有帮助  0</span></li><li class="unlike"><i class="icon unlike-icon"></i> <span id="unlike_num">没帮助 0</span></li></ul></div></div></div><div class="col col--3" style="padding-left:0px"><div class="tableOfContents_k96L thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#目前支持量化的算子" class="table-of-contents__link">目前支持量化的算子</a></li><li><a href="#operator-命名规则" class="table-of-contents__link">Operator 命名规则</a></li><li><a href="#量化方式" class="table-of-contents__link">量化方式</a></li><li><a href="#scale说明" class="table-of-contents__link">Scale说明</a></li><li><a href="#mode参数值" class="table-of-contents__link">Mode参数值</a></li><li><a href="#onnx类型值" class="table-of-contents__link">onnx类型值</a><ul><li><a href="#类型说明" class="table-of-contents__link">类型说明</a></li><li><a href="#类型和值" class="table-of-contents__link">类型和值</a></li><li><a href="#inputs" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs" class="table-of-contents__link">Outputs</a></li><li><a href="#attr" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-1" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-1" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-1" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-1" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-2" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-2" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-2" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-2" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-3" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-3" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-3" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-3" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs1---" class="table-of-contents__link">Inputs（1 - ∞）</a></li><li><a href="#outputs-4" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-4" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-4" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-4" class="table-of-contents__link">Inputs</a></li><li><a href="#venus-limits" class="table-of-contents__link">venus limits</a></li><li><a href="#outputs-5" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-5" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-5" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-5" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-6" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-6" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-6" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-6" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-7" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-7" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-7" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-7" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-8" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-8" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-8" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-8" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-9" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-9" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-9" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-9" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-10" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-10" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-10" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-10" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-11" class="table-of-contents__link">Outputs</a></li><li><a href="#type-constraints-11" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-11" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-12" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-11" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-12" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-12" class="table-of-contents__link">Inputs</a></li><li><a href="#venus-limits-1" class="table-of-contents__link">venus limits</a></li><li><a href="#outputs-13" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-12" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-13" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-13" class="table-of-contents__link">Inputs</a></li><li><a href="#venus-limits-2" class="table-of-contents__link">venus limits</a></li><li><a href="#outputs-14" class="table-of-contents__link">Outputs</a></li><li><a href="#inputs-14" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-15" class="table-of-contents__link">Outputs</a></li><li><a href="#type-constraints-14" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-15" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-16" class="table-of-contents__link">Outputs</a></li><li><a href="#type-constraints-15" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-16" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-17" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-13" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-16" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-17" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-18" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-14" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-17" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-18" class="table-of-contents__link">Inputs</a></li><li><a href="#venus-limits-3" class="table-of-contents__link">venus limits</a></li><li><a href="#outputs-19" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-15" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-18" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-19" class="table-of-contents__link">Inputs</a></li><li><a href="#venus-limits-4" class="table-of-contents__link">venus limits</a></li><li><a href="#outputs-20" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-16" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-19" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-20" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-21" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-17" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-20" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-21" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-22" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-18" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-21" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-22" class="table-of-contents__link">Inputs</a></li><li><a href="#venus-limits-5" class="table-of-contents__link">venus limits</a></li><li><a href="#outputs-23" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-19" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-22" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-23" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-24" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-20" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-23" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-24" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-25" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-21" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-24" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-25" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-26" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-22" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-25" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-26" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-27" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-23" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-26" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-27" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-28" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-24" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-27" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-28" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-29" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-25" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-28" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-29" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-30" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-26" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-29" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-30" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-31" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-27" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-30" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-31" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-32" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-28" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-31" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-32" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-33" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-29" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-32" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-33" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-34" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-30" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-33" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-34" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-35" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-31" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-34" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-35" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-36" class="table-of-contents__link">Outputs</a></li><li><a href="#attr-32" class="table-of-contents__link">Attr</a></li><li><a href="#type-constraints-35" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-36" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-37" class="table-of-contents__link">Outputs</a></li><li><a href="#type-constraints-36" class="table-of-contents__link">Type Constraints</a></li><li><a href="#inputs-37" class="table-of-contents__link">Inputs</a></li><li><a href="#outputs-38" class="table-of-contents__link">Outputs</a></li><li><a href="#type-constraints-37" class="table-of-contents__link">Type Constraints</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 安徽聆思智能科技有限公司皖ICP备05001217号</div></div></div></footer></div>
<script src="/docs-csk6/assets/js/runtime~main.2d336c42.js"></script>
<script src="/docs-csk6/assets/js/main.70a533e0.js"></script>
<script>!function(e,n,s,t,d){e[s]=e[s]||function(){(e[s].a=e[s].a||[]).push(arguments)},(d=n.createElement("script")).async=!0,d.src="https://cdn.jsdelivr.net/simplemde/latest/simplemde.min.js",n.body.appendChild(d)}(window,document,"simplemde")</script></body>
</html>