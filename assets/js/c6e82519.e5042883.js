"use strict";(self.webpackChunkls_docs_web=self.webpackChunkls_docs_web||[]).push([[786],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>d});var i=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},r=Object.keys(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=i.createContext({}),s=function(e){var t=i.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=s(e.components);return i.createElement(c.Provider,{value:t},e.children)},h={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},u=i.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,c=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=s(n),d=a,m=u["".concat(c,".").concat(d)]||u[d]||h[d]||r;return n?i.createElement(m,o(o({ref:t},p),{},{components:n})):i.createElement(m,o({ref:t},p))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,o=new Array(r);o[0]=u;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l.mdxType="string"==typeof e?e:a,o[1]=l;for(var s=2;s<r;s++)o[s]=n[s];return i.createElement.apply(null,o)}return i.createElement.apply(null,n)}u.displayName="MDXCreateElement"},41002:(e,t,n)=>{n.r(t),n.d(t,{contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=n(87462),a=(n(67294),n(3905));const r={sidebar_label:"README_EN",sidebar_position:8},o="![logo](../Thinker/Introduction/files/Thinker_logo.png)",l={unversionedId:"AIEcology/Thinker/README_EN",id:"AIEcology/Thinker/README_EN",isDocsHomePage:!1,title:"![logo](../Thinker/Introduction/files/Thinker_logo.png)",description:"logo",source:"@site/docs/AIEcology/Thinker/README_EN.md",sourceDirName:"AIEcology/Thinker",slug:"/AIEcology/Thinker/README_EN",permalink:"/docs-csk6/AIEcology/Thinker/README_EN",version:"current",sidebarPosition:8,frontMatter:{sidebar_label:"README_EN",sidebar_position:8},sidebar:"AIEcologyThinker",previous:{title:"README",permalink:"/docs-csk6/AIEcology/Thinker/readme"}},c=[{value:"logo",id:"logo",children:[]},{value:"Frame Features:",id:"frame-features",children:[{value:"1. Ultra-lightweight",id:"1-ultra-lightweight",children:[]},{value:"2. Generality",id:"2-generality",children:[]},{value:"3. High Performance",id:"3-high-performance",children:[]}]},{value:"Quick Start",id:"quick-start",children:[{value:"1. Tool chain installation",id:"1-tool-chain-installation",children:[]},{value:"2. Model design",id:"2-model-design",children:[]},{value:"3. Quantitative training and export of models",id:"3-quantitative-training-and-export-of-models",children:[]},{value:"4. Model analysis and packaging",id:"4-model-analysis-and-packaging",children:[]},{value:"5. Inference execution",id:"5-inference-execution",children:[]},{value:"6. Auxiliary Functions",id:"6-auxiliary-functions",children:[]}]},{value:"Ability Demonstration",id:"ability-demonstration",children:[]},{value:"Communication and Feedback",id:"communication-and-feedback",children:[]},{value:"Citation",id:"citation",children:[]},{value:"Copyright and License",id:"copyright-and-license",children:[]}],s={toc:c};function p(e){let{components:t,...r}=e;return(0,a.kt)("wrapper",(0,i.Z)({},s,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:""}),(0,a.kt)("h2",{id:"logo"},(0,a.kt)("img",{alt:"logo",src:n(18809).Z})),(0,a.kt)("h4",{id:"english--chinese"},"English | ",(0,a.kt)("a",{parentName:"h4",href:"/docs-csk6/AIEcology/Thinker/readme"},"Chinese")),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://pypi.org/project/thinker"},(0,a.kt)("img",{parentName:"a",src:"https://img.shields.io/pypi/pyversions/thinker.svg",alt:"PyPI - Python Version"})),"\n",(0,a.kt)("a",{parentName:"p",href:"https://badge.fury.io/py/thinker"},(0,a.kt)("img",{parentName:"a",src:"https://badge.fury.io/py/thinker.svg",alt:"PyPI"})),"\n",(0,a.kt)("a",{parentName:"p",href:"https://pepy.tech/project/thinker"},(0,a.kt)("img",{parentName:"a",src:"https://pepy.tech/badge/thinker",alt:"Downloads"})),"\n",(0,a.kt)("a",{parentName:"p",href:"https://hub.docker.com/r/thinker/thinker-cpu"},(0,a.kt)("img",{parentName:"a",src:"https://img.shields.io/docker/pulls/thinker/thinker-cpu.svg",alt:"DockerHub"})),"\n",(0,a.kt)("a",{parentName:"p",href:"https://github.com/LISTENAI/thinker/blob/main/LICENSE"},(0,a.kt)("img",{parentName:"a",src:"https://img.shields.io/github/license/thinker-ai/thinker.svg?style=flat-square",alt:"LICENSE"}))),(0,a.kt)("h1",{id:"welcome-to-the-thinker-github"},"Welcome to the Thinker GitHub"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Thinker is based on the AIOT chip CSK60XX independently developed by LISTENAI Technology, combined with another open source quantitative training tool, Linger, can implement an industrial-grade deep learning platform, integrating a deep learning quantitative training and inference framework, LUNA device library, and rich tool components."),(0,a.kt)("li",{parentName:"ul"},"Thinker helps developers quickly launch AI services on the VENUS chip, helping more and more AIOT products to achieve AI empowerment and realize industrial intelligent upgrades. At present, the linger+thinker tool chain has supported more than 10 AI applications of linger chips in the fields of computer vision, voice wake-up, speech recognition, offline translation, etc.")),(0,a.kt)("hr",null),(0,a.kt)("h2",{id:"frame-features"},"Frame Features:"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"thinker/docs/images/struct.png",src:n(95924).Z})),(0,a.kt)("h3",{id:"1-ultra-lightweight"},"1. Ultra-lightweight"),(0,a.kt)("p",null,"As shown in the above framework diagram, the Thinker framework contains two parts: offline analysis tool and engine executor The offline analysis tool contains most of the computational graph pre-processing part, including graph fusion, graph optimization and graph adaptation functions, simulating the executor functions, allocating memory in advance, and stripping the non-computational part of the executor functions as much as possible. The engine executor is mainly responsible for the computational part and other auxiliary debugging functions (optional). The code is streamlined and implemented in pure C language without any dependencies, and it can be easily deployed to CSKXX devices with basically no modifications using the research sample demos."),(0,a.kt)("h3",{id:"2-generality"},"2. Generality"),(0,a.kt)("p",null,"For regular CV models, after linger's quantization training to export computational graphs, one key package is deployed. It supports multiple input and multiple output computation graphs, dynamic input (variable input size), and 32 common quantization operators in CV models, see the operator ",(0,a.kt)("a",{parentName:"p",href:"/docs-csk6/AIEcology/Thinker/Inference_Engine/operator"},"support list")," for details."),(0,a.kt)("h3",{id:"3-high-performance"},"3. High Performance"),(0,a.kt)("p",null,"The engine actuator is specially adapted for the VENUS architecture of CSK60XX, integrating the LUNA library for core computing, giving full play to the arithmetic power of LUNA by handwriting custom instruction codes, and running common CV models under single thread can approach the peak arithmetic power of the device. The thinker+linger tool chain supports full low-precision computation (int8/int16) to improve inference performance, and adapts the relevant instructions. Compared to floating-point models, quantized models can reduce the number of parameters by 50%-75%, speeding up data access and improving computing efficiency."),(0,a.kt)("hr",null),(0,a.kt)("h2",{id:"quick-start"},"Quick Start"),(0,a.kt)("p",null,"The Linger tool chain includes Linger and Thinker, which are interlinked and must be used jointly; Thinker relies on Linger's computational graph export, and both use the same standard library of operators.\nThe entire tool chain is used throughout the life cycle of the model landing and can be roughly divided into six phases."),(0,a.kt)("h3",{id:"1-tool-chain-installation"},"1. Tool chain installation"),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"/docs-csk6/AIEcology/Thinker/Introduction/env"},"Environment Configuration"),"\uff1aSupport multiple installation methods such as pip, source code and docker"),(0,a.kt)("h3",{id:"2-model-design"},"2. Model design"),(0,a.kt)("p",null,"After finishing the model structure design, algorithm researchers use random initialization parameters to go through the linger+thinker tool chain, which evaluates the model's parameter adaptability, memory consumption and running efficiency to avoid design rework later on when the application needs are not met."),(0,a.kt)("h3",{id:"3-quantitative-training-and-export-of-models"},"3. Quantitative training and export of models"),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://github.com/LISTENAI/linger"},"Linger")," is a plug-in for pytorch and can be imported with one click. ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/LISTENAI/linger"},"Linger")," uses QAT quantization, which is completely or basically lossless for CV models. After the quantization training is completed, the model can be exported with a single click using its own tools."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs-csk6/AIEcology/Thinker/Inference_Engine/model_quant"},"Example Tutorial 3"))),(0,a.kt)("h3",{id:"4-model-analysis-and-packaging"},"4. Model analysis and packaging"),(0,a.kt)("p",null,"Parameter checking of the computational graph, computational graph optimization and memory analysis checking using Thinker's offline tool tpacker. Finally, the computational graph is serialized into the format required by the engine executor and the runtime memory is pre-allocated."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs-csk6/AIEcology/Thinker/Inference_Engine/thinker_packer"},"Example Tutorial 4"))),(0,a.kt)("h3",{id:"5-inference-execution"},"5. Inference execution"),(0,a.kt)("p",null,"Directly load resources serialized by offline tools. Implement computational graphs on VENUS chips with few or even zero modifications."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs-csk6/AIEcology/Thinker/Example/example"},"Example Tutorial 5"))),(0,a.kt)("h3",{id:"6-auxiliary-functions"},"6. Auxiliary Functions"),(0,a.kt)("p",null,"View operator performance statistics and intermediate result data"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs-csk6/AIEcology/Thinker/Tools/tool"},"Example Tutorial 6"))),(0,a.kt)("hr",null),(0,a.kt)("h2",{id:"ability-demonstration"},"Ability Demonstration"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs-csk6/AIEcology/Thinker/Inference_Engine/Thinke_api"},"API Interface")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"/docs-csk6/AIEcology/Thinker/Inference_Engine/operator"},"Support quantization OP list and restriction description"))),(0,a.kt)("hr",null),(0,a.kt)("h2",{id:"communication-and-feedback"},"Communication and Feedback"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Welcome to submit bugs and suggestions through Github Issues"),(0,a.kt)("li",{parentName:"ul"},"Technical Exchange WeChat Group  ")),(0,a.kt)("hr",null),(0,a.kt)("h2",{id:"citation"},"Citation"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://github.com/onnx/onnx"},"ONNX")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://github.com/alibaba/MNN"},"MNN")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://github.com/Tencent/ncnn"},"NCNN")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://github.com/Tencent/TNN"},"TNN"))),(0,a.kt)("hr",null),(0,a.kt)("h2",{id:"copyright-and-license"},"Copyright and License"))}p.isMDXComponent=!0},18809:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/Thinker_logo-090ed475d25121c65a61914f7bbee76c.png"},95924:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/struct-81d784654d0baa766a7044305980657b.png"}}]);